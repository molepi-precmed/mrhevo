
#' return upper bound on p-value  where z is so extreme that pnorm(-z) returns zero
#'
#' @param z Standard normal deviate.
#' @returns A string in scientific notation.
pnorm.extreme <- function(z, upper=TRUE) {
    ## https://www.johndcook.com/blog/norm-dist-bounds/
    if(upper) { # upper bound on log10 tail prob
        c <- 8 / pi
    } else { # lower bound
        c = 4
    }
    x <-  2 * sqrt(2 / pi) / (z + sqrt(z^2 + c))
    ln_p = -0.5 * z^2 + log(x)
    log10p <- ln_p / log(10)
    exponent <- floor(log10p)
    coeff <- 10^(log10p - exponent)
    string <- paste0(round(coeff), "E", exponent)
    return(string)
}

#' reformat for LaTeX p-values that are in scientific notation
#'
#' @param x Character vector of numbers in scientific notation.
#' @returns A character vector of LaTeX math expressions.  
format.scinot.pvalue <- function(x, nexp=1) {
    x <- toupper(x)
    x.split <- as.numeric(unlist(strsplit(as.character(x), "E"))) # split x.split at E
    x.split <- signif(as.numeric(x.split, 1))
    x.split <- t(matrix(x.split, nrow=2))
    roundedto10 <- x.split[, 1]==10
    x.split[, 1][roundedto10] <- 1
    x.split[, 2][roundedto10] <- x.split[, 2][roundedto10] - 1
    p.latex <- sprintf("\\ensuremath{%.*f \\times 10^{%0*d}}", 0, x.split[, 1], nexp, x.split[, 2])
    return(p.latex)
}

#' generate formatted p-values from z values
#'
#' @param z Standard normal deviate.
#' @param sigfig Number of significant figures to show in p-values. 
#' @param neglogp.threshold.scinot Minus log10(pvalue) threshold for scientific notation.
#' @param neglogp.threshold Minus log10(pvalue) cutoff for thresholding p-values.
#' @param returns Vector of formatted p-values. 
format.z.aspvalue <- function(z, sigfig=1, neglogp.threshold.scinot=3, neglogp.threshold=NULL) {

    p <- signif(2 * pnorm(-abs(z)), sigfig)
    p.char <- toupper(as.character(p))
    ## pnorm.extreme returns a character string of form "NE-NNN"
    p.char[!is.na(p.char) & p.char=="0"] <- pnorm.extreme(z[!is.na(p.char) & p.char=="0"])    # where R outputs 0

    ## revert from scientific notation p-values above threshold for neglogp.threshold.scinot
    sci.revert <- grepl("E", p.char) & p > 10^-neglogp.threshold.scinot
    p.char[sci.revert] <-  format(p[sci.revert], scientific=FALSE)

    if(!is.null(neglogp.threshold)) { # thresholding of p values
        p.char[p < 10^-neglogp.threshold] <- paste0("<",
                                                    format(10^-neglogp.threshold,
                                                           scientific=FALSE))
    }
    ## format values in scientific notation for LaTeX
    p.char[grep("E", p.char)] <- format.scinot.pvalue(p.char[grep("E", p.char)])
    return(p.char)
}

#' boostrap standard error for weighted median estimate of causal effect from summary statistics for two-step Mendelian randomization
#'
#' @returns Standard error of weighted median estimate
weighted.median.boot <- function(betaXG, betaYG, sebetaXG, sebetaYG, weights) {
    med = numeric(1000)
    for(i in 1:1000){
        betaXG.boot = rnorm(length(betaXG), mean=betaXG, sd=sebetaXG)
        betaYG.boot = rnorm(length(betaYG), mean=betaYG, sd=sebetaYG)
        betaIV.boot = betaYG.boot / betaXG.boot
        med[i] = matrixStats::weightedMedian(betaIV.boot, weights)
    }
    return(sd(med))
}

#' calculate summary stats for second step of two-step Mendelian randomization
#'
#' @param Y Integer vector of binary outcome or numeric vector of continuous outcome.
#' @param Z Data.table of genetic instruments.
#' @param X_u Data.table of covariates.
#' @returns Data.table of coefficients for each genetic instrument
get_summarystatsforMR <- function(Y, Z, X_u) {
    require(data.table)
    YXZ.dt <- data.table(y=Y, Z, X_u)
    
    ## loop over instruments to fit regression of Y on Z, adjusted for X_u
    coeffs <- NULL
    for(scoreid in colnames(Z)) {
        formula.string <- paste0("y ~ ",
                                 paste(colnames(X_u), collapse=" + "),
                                 " + ", scoreid)
        coeff <- summary(glm(data=YXZ.dt, formula=as.formula(formula.string),
                             family="binomial"))$coefficients
        coeff <- as.data.table(coeff, keep.rownames="variable")
        coeff <- coeff[variable==scoreid]
        coeffs <- rbind(coeffs, coeff)
    }
    colnames(coeffs) <- c("scoreid", "beta_YZ", "SE_YZ", "z", "p")
    return(coeffs)
}

#' calculate conventional MR estimators from summary stats
#'
#' @param coeffs Data.table generated by get_summarystatsforMR with additional columns for coefficients of regression of exposure on instrument (alpha_hat) and their standard errors (se.alphahat).
#' @returns Table of estiamtes for weighted mean, weighted median, and penalized weighted median of instrumental variable estimates. 
get_estimatorsMR <- function(coeffs) {
    coeffs[, thetaIV := beta_YZ / alpha_hat]  # ratio estimates

    ## weighted median method
    coeffs[, weights := (SE_YZ / alpha_hat)^-2]  
    thetaIVW <- coeffs[, sum(thetaIV * weights) / sum(weights)] # IVW estimate
    se.thetaIVW  <- coeffs[, sqrt(1 / sum(weights))]
    ## penalty for outliers
    coeffs[, penalty := pchisq(weights * (thetaIV - thetaIVW)^2, df=1, lower.tail=FALSE)]
    coeffs[, penalized.weights := weights * pmin(1, penalty * .N)]  # penalized weights
    
    thetaWM <- coeffs[, matrixStats::weightedMedian(x=thetaIV, w=weights)] # weighted median estimate
    se.thetaWM <- coeffs[, weighted.median.boot(alpha_hat, beta_YZ, se.alpha_hat, SE_YZ, weights)]
    
    thetaPWM <- coeffs[, matrixStats::weightedMedian(x=thetaIV, w=penalized.weights)] # penalized weighted median estimate
    se.thetaPWM <- coeffs[, weighted.median.boot(alpha_hat, beta_YZ,
                                                 se.alpha_hat, SE_YZ,
                                                 penalized.weights)]
    
    estimators.dt <- data.table(method=c("Weighted mean", "Weighted median",
                                         "Penalized weighted median"),
                                Estimate=c(thetaIVW, thetaWM, thetaPWM), 
                                SE=c(se.thetaIVW, se.thetaWM, se.thetaPWM))
    estimators.dt[, z := Estimate / SE]
    estimators.dt[, pvalue := 2 * pnorm(-abs(z))]
    estimators.dt[, pvalue.formatted := format.z.aspvalue(z)]
    return(estimators.dt)
}

#' calculate maximum likelihood estimate and p-value from posterior samples and prior
#' @param x Numeric vector of samples from the posterior distribution of the parameter.
#' @param prior Numeric vector of values of the prior density for each element of x.
#' @param return.asplot Logical value determines whether the function returns maximum likelihood estimate or a plot of the posterior density and log-likelihood.
#' @returns If return.asplot is FALSE, data.table with one row containing maximum likelihood estimate, standard error, test statistic, p-value, formatted p-value. 
mle.se.pval <- function(x, prior, return.asplot=FALSE) {
    require(data.table)
    require(ggplot2)
      
    invprior <- 1 / prior
    invprior <- invprior / sum(invprior)

    ## likelihood is posterior density divided by prior
    ## equivalently we can weight posterior samples by inverse of the prior
    ## when fitting a kernel density (usually Sheather-Jones is preferred to default bw)
    ## wider bandwidth gives better approximation to quadratic
    lik <- density(x, bw="SJ", adjust=2, weights=invprior)
    logl <- log(lik$y)
    xvals <- lik$x
    xvals.sq <- lik$x^2
    ## possible refinement would be to weight the regression that is used to fit the quadratic approximation:
    fit.quad <- lm(logl ~ xvals + xvals.sq)
    a <- -as.numeric(fit.quad$coefficients[3]) # y = -a * x^2 + bx
    b <- as.numeric(fit.quad$coefficients[2]) # mle b/2a, se sqrt(1/2a)
    mle <- 0.5 * b / a
    stderr <- sqrt(0.5 / a)
    z <- mle / stderr
    pvalue <- 2 * pnorm(-abs(z))
    pvalue.formatted <- format.z.aspvalue(z)
    if(return.asplot) {
        loglik.dt <- data.table(logl.fit=logl, x=xvals, logl.quad=-a * xvals^2 + b * xvals)
        loglik.dt[, rownum := .I]
        loglik.dt[, logl.fit := logl.fit - max(logl.fit)]
        loglik.dt[, logl.quad := logl.quad - max(logl.quad)]
        loglik.long <- melt(loglik.dt, id.vars="rownum", measure.vars=c("logl.fit", "logl.quad"),
                            variable.name="curve", value.name="loglik")
        loglik.long <- loglik.dt[, .(rownum, x)][loglik.long, on="rownum"]
        loglik.long[, curve := car::recode(curve,
                                           "'logl.fit'='Smoothed curve'; 'logl.quad'='Fitted quadratic'", as.factor=TRUE)]
        p.loglik <- ggplot(loglik.long, aes(x=x, y=loglik, color=curve)) +
            geom_line() +
            xlab("Parameter value") + 
            ylab("Log-likelihood") +
            theme(legend.position=c(0.5, 0.5)) +
            theme(legend.title=element_blank())
        xlimits <- c(min(loglik.long$x), max(loglik.long$x))
        fitted <- density(x)
        fitted.dt <- data.table(x=fitted$x, posterior=fitted$y)
        p.density <- ggplot(fitted.dt, aes(x=x, y=posterior)) +
            geom_line() +
            xlab("Parameter value") + 
            ylab("Smoothed posterior density") + 
            scale_x_continuous(limits=xlimits)
        
        p.bayesloglik <- cowplot::plot_grid(p.density, p.loglik, nrow=2)
        return(p.bayesloglik)
    } else {
        return(data.table(Estimate=mle, SE=stderr, z=z, pvalue=pvalue, pvalue.formatted=pvalue.formatted))
    }
}

#' run Stan model for Mendelian randomization with regularized horsehoe prior on pleiotropic effects
#' @param sampling If TRUE, uses sampling rather than variational approximation.
#' @param logistic If TRUE, uses logistic rather than linear regression.
#' @param Z Data.table of genetic instruments..
#' @param Y Vector of outcomes.  
#' @param sigma_y Standard deviation of outcome variable, used only if logistic=FALSE.
#' @param X_u Data.table of unpenalized covariates.
#' @param alpha_hat Vector of estimated coefficients for effect of instruments on exposure.  
#' @param se.alpha_hat Vector of standard errors for coefficients alpha_hat.
#' @param fraction_pleio Prior guess at fraction of instruments that have pleiotropic effects: values between 0.05 and 0.95 are allowed. 
#' @param priorsd_theta Standard deviation of prior on theta.
#' @returns An object of class stanfit. 
run_mrhevo <- function(use.sampling=TRUE, logistic=TRUE, Z, Y, sigma_y=1, X_u, alpha_hat, se.alpha_hat, fraction_pleio=NULL, priorsd_theta=1) {
    require(rstan)

    options(mc.cores = parallel::detectCores())
    rstan_options(auto_write = TRUE)

    cat("compiling stan model ... ")
    mr.stanmodel <- stan_model(file="../mrhevo/MRHevo_logistic.stan",
                               model_name="MRHevo.logistic", verbose=FALSE)
    cat("done\n")

    ## check arguments for consistency
    stopifnot(length(unique(c(nrow(Z), nrow(X_u), length(Y)))) == 1)
    stopifnot(length(unique(c(ncol(Z), length(alpha_hat), length(se.alpha_hat)))) == 1)
    stopifnot(fraction_pleio >= 0.05 & fraction_pleio <= 0.95)

    N <- nrow(Z)
    J <- ncol(Z)
    
    ## priors
    scale_intercept_y <- 10  # weak prior on Y intercept
    scale_unpenalized <- 1 # prior on coeffs for unpenalized covariates X_u
    ## prior on c 
    slab_scale <- 0.25 # small value regularizes largest coeffs 
    slab_df <- 2    # 1 for half-Cauchy, large value specifies a gaussian prior on slab component
    nu_global <- 1  # 1 for half-Cauchy prior on global scale param: specifying a larger value will limit the narrowness of the spike component 
    nu_local <- 1    # 1 for half-Cauchy, horseshoe+ or horseshoe if c is large
    
    if(is.null(fraction_pleio)) {
        fraction_pleio <- 0.5 # prior guess of number of instruments that are pleiotropic
    } 
    r_pleio <- fraction_pleio * J
    
    ## Piironen and Vehtari recommend that the prior on tau should be chosen to have most of the prior mass
    ## near (r_pleio / (N * (J - r_pleio)) * sqrt(pseudovariance) / sqrt(N),
    ## where r_pleio is a prior guess for the number of nonzero coefficients 

    ## prior median of a half-t distribution with nu_global df
    priormedian <- qt(p=0.75, df=nu_global, lower.tail=TRUE) # 0.82 with nu_global=1)
    if(logistic) {
        mu <- mean(Y)
        pseudovariance <- (mu * (1 - mu))^-1
        tau_0 <- (r_pleio / (J - r_pleio)) * sqrt(pseudovariance) / sqrt(N)
    } else {
        tau_0 <- (r_pleio / (J - r_pleio)) * sigma_y / sqrt(N)
    }
    ## choose prior on tau so that most of the prior mass is near tau_0
    ## choose scale_global so that the prior median of the half-t distribution equates to tau_0
    scale_global <- tau_0 / priormedian 

    ## sample the posterior
    data.stan <- list(logistic=as.integer(logistic),
                      Z=as.matrix(Z), Y=Y, X_u=as.matrix(X_u),   
                      N=N, J=J, U=ncol(X_u),
                      alpha_hat=alpha_hat,
                      sd_alpha_hat=se.alpha_hat,
                      nu_global=nu_global,
                      nu_local=nu_local,
                      scale_global=scale_global,
                      scale_intercept_y=scale_intercept_y,
                      priorsd_theta=priorsd_theta,
                      slab_scale=slab_scale,
                      slab_df=slab_df, priorsd_theta=priorsd_theta)

    if(use.sampling) {
        fit.mc <- rstan::sampling(object=mr.stanmodel,
                                  data=data.stan,
                                  iter=1200, warmup=400,
                                  cores=4,
                                  chains=4,
                                  refresh=400,
                                  control=list(adapt_delta=0.95),
                                  verbose=FALSE) 
    } else {
        fit.mc <- rstan::vb(object=mr.stanmodel,
                            data=data.stan,
                            algorithm="meanfield",
                            refresh=5000,
                            iter=20000,
                            adapt_engaged=TRUE,
                            tol_rel_obj=0.005)
    }    
    return(fit.mc)
}
